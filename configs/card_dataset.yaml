num_workers: 4
batch_size: 8
num_train_epochs: 15

cardDataset:
  root: './data'
  raw: 'AllPrintings.json'
  db: 'AllCards.json'

  features: ['name', 'ctype', 'text', 'manaCost', 'toughness', 'power']
  tokenizer: "EleutherAI/gpt-neo-125M" # 'EleutherAI/gpt-neo-1.3B'
  max_tokens_card: 300 # Used to reduce cards
  context_window: &cw 150

  token_ids:
    # additional_special_tokens: ['[CLONE]', '[MASK]', '[SEP]']
    pad_token: '<|pad|>'
 
  summarize_model:
    chatai_parms: 
      model_name: "gpt-3.5-turbo-0613"
      temperature: 0
      max_tokens: *cw
    
    pre_prompt: "./dataset/summarize_prompt.txt"


deckDataset:
  root: './data'
  json_name: 'AllDecks.npy'
  deck_length: 60
  mask_amount: 10
  context_window: *cw

model:
  name: "EleutherAI/gpt-neo-125M"

optimizer:
  lr: &blr 1.0e-6
  weight_decay: 0.05

coeff_step_size_up: 0.75
coeff_step_size_down: 19

schedular:
  base_lr: *blr
  max_lr: 1.0e-4
  mode: 'triangular2'
  
  cycle_momentum: false

Resume:
  chk_ptn: './output/checkpoint-46000'