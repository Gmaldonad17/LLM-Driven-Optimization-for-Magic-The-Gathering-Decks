num_workers: 4
batch_size: 8
num_train_epochs: 20

cardDataset:
  root: './data'
  raw: 'AllPrintings.json'
  db: 'AllCards.json'

  features: ['name', 'ctype', 'text', 'manaCost', 'toughness', 'power']
  tokenizer: "EleutherAI/gpt-neo-125M" # 'EleutherAI/gpt-neo-1.3B'
  max_tokens_card: 300 # Used to reduce cards
  context_window: &cw 150

  token_ids:
    # additional_special_tokens: ['[CLONE]', '[MASK]', '[SEP]']
    pad_token: '<|pad|>'
 
  summarize_model:
    chatai_parms: 
      model_name: "gpt-3.5-turbo-0613"
      temperature: 0
      max_tokens: *cw
    
    pre_prompt: "./dataset/summarize_prompt.txt"


deckDataset:
  root: './data'
  json_name: 'AllDecks.npy'
  deck_length: 60
  mask_amount: 10
  context_window: *cw


model:
  name: "EleutherAI/gpt-neo-125M"


Resume:
  chk_ptn: null # Str or null
  id: ''